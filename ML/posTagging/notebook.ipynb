{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install flair numpy sklearn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 13:48:10,955 Reading data from C:\\Users\\sudip\\.flair\\datasets\\ud_english\n",
      "2022-07-02 13:48:10,956 Train: C:\\Users\\sudip\\.flair\\datasets\\ud_english\\en_ewt-ud-train.conllu\n",
      "2022-07-02 13:48:10,956 Dev: C:\\Users\\sudip\\.flair\\datasets\\ud_english\\en_ewt-ud-dev.conllu\n",
      "2022-07-02 13:48:10,956 Test: C:\\Users\\sudip\\.flair\\datasets\\ud_english\\en_ewt-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "import flair.datasets\n",
    "corpus = flair.datasets.UD_ENGLISH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 12543 train + 2001 dev + 2077 test sentences\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What followed the next day is considered by the governor and Dunn to be the pivotal moment of the energy crisis .\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[5132].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"What followed the next day is considered by the governor and Dunn to be the pivotal moment of the energy crisis .\" â†’ [\"What\"/WP, \"followed\"/VBD, \"the\"/DT, \"next\"/JJ, \"day\"/NN, \"is\"/VBZ, \"considered\"/VBN, \"by\"/IN, \"the\"/DT, \"governor\"/NN, \"and\"/CC, \"Dunn\"/NNP, \"to\"/TO, \"be\"/VB, \"the\"/DT, \"pivotal\"/JJ, \"moment\"/NN, \"of\"/IN, \"the\"/DT, \"energy\"/NN, \"crisis\"/NN, \".\"/.]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[5132].to_tagged_string('pos'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 13:48:22,143 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12543it [00:00, 24837.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 13:48:22,652 Dictionary created for label 'upos' with 18 values: NOUN (seen 34761 times), PUNCT (seen 23620 times), VERB (seen 22946 times), PRON (seen 18589 times), ADP (seen 17730 times), DET (seen 16314 times), ADJ (seen 13167 times), AUX (seen 12440 times), PROPN (seen 12345 times), ADV (seen 9462 times), CCONJ (seen 6690 times), PART (seen 5745 times), SCONJ (seen 4554 times), NUM (seen 4119 times), X (seen 704 times), SYM (seen 698 times), INTJ (seen 694 times)\n",
      "Dictionary with 18 tags: <unk>, NOUN, PUNCT, VERB, PRON, ADP, DET, ADJ, AUX, PROPN, ADV, CCONJ, PART, SCONJ, NUM, X, SYM, INTJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "upos_dictionary = corpus.make_label_dictionary(label_type='upos')\n",
    "print(upos_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what PRON\n",
      "follow VERB\n",
      "the DET\n",
      "next ADJ\n",
      "day NOUN\n",
      "be AUX\n",
      "consider VERB\n",
      "by ADP\n",
      "the DET\n",
      "governor NOUN\n",
      "and CCONJ\n",
      "Dunn PROPN\n",
      "to PART\n",
      "be AUX\n",
      "the DET\n",
      "pivotal ADJ\n",
      "moment NOUN\n",
      "of ADP\n",
      "the DET\n",
      "energy NOUN\n",
      "crisis NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in corpus.train[5132].tokens:\n",
    "  print(token.tag, token.labels[1].value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17022"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = set()\n",
    "for _corpus in [corpus.train, corpus.dev, corpus.test]:\n",
    "  for sentence in _corpus:\n",
    "    for token in sentence.tokens:\n",
    "      words.add(token.tag)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class GensimWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    gensim's own gensim.sklearn_api.W2VTransformer doesn't support out of vocabulary words,\n",
    "    hence we create our own\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None,\n",
    "                 sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5,\n",
    "                 ns_exponent=0.75, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,\n",
    "                 callbacks=(), max_final_vocab=None):\n",
    "        self.size = size\n",
    "        self.alpha = alpha\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sample = sample\n",
    "        self.seed = seed\n",
    "        self.workers = workers\n",
    "        self.min_alpha = min_alpha\n",
    "        self.sg = sg\n",
    "        self.hs = hs\n",
    "        self.negative = negative\n",
    "        self.ns_exponent = ns_exponent\n",
    "        self.cbow_mean = cbow_mean\n",
    "        self.hashfxn = hashfxn\n",
    "        self.iter = iter\n",
    "        self.null_word = null_word\n",
    "        self.trim_rule = trim_rule\n",
    "        self.sorted_vocab = sorted_vocab\n",
    "        self.batch_words = batch_words\n",
    "        self.compute_loss = compute_loss\n",
    "        self.callbacks = callbacks\n",
    "        self.max_final_vocab = max_final_vocab\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model_ = Word2Vec(\n",
    "            sentences=X, corpus_file=None,\n",
    "            size=self.size, alpha=self.alpha, window=self.window, min_count=self.min_count,\n",
    "            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.seed,\n",
    "            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.hs,\n",
    "            negative=self.negative, ns_exponent=self.ns_exponent, cbow_mean=self.cbow_mean,\n",
    "            hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word,\n",
    "            trim_rule=self.trim_rule, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words,\n",
    "            compute_loss=self.compute_loss, callbacks=self.callbacks,\n",
    "            max_final_vocab=self.max_final_vocab)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_embeddings = np.array([self._get_embedding(words) for words in X])\n",
    "        return X_embeddings\n",
    "\n",
    "    def _get_embedding(self, words):\n",
    "        valid_words = [word for word in words if word in self.model_.wv.vocab]\n",
    "        if valid_words:\n",
    "            embedding = np.zeros(\n",
    "                (len(valid_words), self.size), dtype=np.float32)\n",
    "            for idx, word in enumerate(valid_words):\n",
    "                embedding[idx] = self.model_.wv[word]\n",
    "\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_word2vec_tr = GensimWord2VecVectorizer(\n",
    "    size=10, min_count=3, sg=1, alpha=0.025, iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for sentence in corpus.train:\n",
    "  for token in sentence.tokens:\n",
    "    X_train.append(token.tag)\n",
    "    y_train.append(token.labels[1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GensimWord2VecVectorizer(iter=10, min_count=3, sg=1, size=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_word2vec_tr.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_word2vec_tr.model_.save('word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('reestablish',\n",
       " array([-0.6197692 ,  0.05817069,  0.01561817,  0.23214982,  0.12287433,\n",
       "        -0.23274094,  1.009097  ,  0.19509827,  0.48299617, -0.20273222],\n",
       "       dtype=float32),\n",
       " 'VERB')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5123], gensim_word2vec_tr.transform([X_train[5123]])[0], y_train[5123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26778048,  0.06921566,  0.4347147 , ...,  0.69781482,\n",
       "         0.16398181,  0.02340554],\n",
       "       [ 0.01692152, -2.08052397,  0.94598621, ..., -1.69790184,\n",
       "         0.52791226,  0.26770422],\n",
       "       [-0.14903422,  0.06417719,  0.3466683 , ...,  0.24111137,\n",
       "         0.28475243, -0.12943198],\n",
       "       ...,\n",
       "       [-0.5206911 , -0.21647924,  0.26263666, ...,  0.18457679,\n",
       "         0.43908226, -0.17785431],\n",
       "       [ 0.57867116, -0.57820368, -1.23070264, ..., -0.81327367,\n",
       "        -0.67024243, -0.34505662],\n",
       "       [-0.0124165 , -0.43035585, -0.9793402 , ...,  0.05271059,\n",
       "        -1.1423316 , -0.2548157 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings = gensim_word2vec_tr.transform(X_train)\n",
    "X_train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for sentence in corpus.test:\n",
    "  for token in sentence.tokens:\n",
    "    X_test.append(token.tag)\n",
    "    y_test.append(token.labels[1].value)\n",
    "    \n",
    "X_test_embeddings = gensim_word2vec_tr.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, max_iter=1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(alpha=.01, max_iter=1000)\n",
    "nn.fit(X_train_embeddings, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.31      0.42      1786\n",
      "         ADP       0.82      0.85      0.84      2029\n",
      "         ADV       0.65      0.54      0.59      1126\n",
      "         AUX       0.90      0.88      0.89      1507\n",
      "       CCONJ       0.99      0.98      0.99       738\n",
      "         DET       0.95      0.94      0.95      1897\n",
      "        INTJ       0.91      0.43      0.59       120\n",
      "        NOUN       0.59      0.84      0.69      4132\n",
      "         NUM       0.94      0.93      0.93       541\n",
      "        PART       0.74      1.00      0.85       649\n",
      "        PRON       0.94      0.89      0.91      2158\n",
      "       PROPN       0.89      0.89      0.89      1983\n",
      "       PUNCT       0.99      1.00      0.99      3098\n",
      "       SCONJ       0.58      0.47      0.52       445\n",
      "         SYM       0.94      0.71      0.81       106\n",
      "        VERB       0.71      0.57      0.63      2641\n",
      "           X       0.75      0.47      0.58       138\n",
      "\n",
      "    accuracy                           0.80     25094\n",
      "   macro avg       0.82      0.75      0.77     25094\n",
      "weighted avg       0.80      0.80      0.79     25094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test_pred = nn.predict(X_test_embeddings)\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn_model.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(nn, 'nn_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b419fc17cb99a387ec03335e5db2015b25fcd2303fb056216a2ace7e886e455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
